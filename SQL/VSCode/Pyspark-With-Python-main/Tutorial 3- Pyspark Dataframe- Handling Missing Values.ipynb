{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ded0783b",
   "metadata": {},
   "source": [
    "### Pyspark Handling Missing Values\n",
    "- Dropping Columns\n",
    "- Dropping Rows\n",
    "- Various Parameter In Dropping functionalities\n",
    "- Handling Missing values by Mean, Median And Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "805e7382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e48ebc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.csv('test2.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53ab7bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed677b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc910fad",
   "metadata": {},
   "source": [
    "## drop the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "523d3c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------+\n",
      "| age|Experience|Salary|\n",
      "+----+----------+------+\n",
      "|  31|        10| 30000|\n",
      "|  30|         8| 25000|\n",
      "|  29|         4| 20000|\n",
      "|  24|         3| 20000|\n",
      "|  21|         1| 15000|\n",
      "|  23|         2| 18000|\n",
      "|null|      null| 40000|\n",
      "|  34|        10| 38000|\n",
      "|  36|      null|  null|\n",
      "+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.drop('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c041e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80a14b3",
   "metadata": {},
   "source": [
    "## To drop any rows containing null values., It reomoves the enitre row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36845e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d1cdcf",
   "metadata": {},
   "source": [
    "### To drop null values using how==any . Removes entire row if a row contains even 1 null, entire row gets removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "156e41cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how=\"any\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf3742b",
   "metadata": {},
   "source": [
    "### To drop null values using how==all . Removes entire row if a row contains null values for all the column, entire row gets removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18cfaa7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how=\"all\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473825a0",
   "metadata": {},
   "source": [
    "##### threshold - This thresh parameter allows for removal of rows with null values where the amount of null is greater than threshold. Suppose a row contains 5 columns, so if the threshold is set to 2, then the column shouldn't have more than 2 null values otherwise the column will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9af0da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "|     null| 34|        10| 38000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how=\"any\",thresh=3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67b5042",
   "metadata": {},
   "source": [
    "##### Subset - Supose, I wish to remove nulls from only a certain set of columns, then we subset parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "787fc949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how=\"any\",subset=['Name']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c3c918",
   "metadata": {},
   "source": [
    "##### Fill - Supose, I wish to fill the nulls with a specific value or default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72bad9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill('Missing Values',['Experience','age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64e01bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b66832fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9602ec4",
   "metadata": {},
   "source": [
    "### How to calculate Mean of the columns. We will use imputer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0efaede",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\this pc\\Pictures\\git repo\\Pyspark-With-Python-main\\Pyspark-With-Python-main\\Tutorial 3- Pyspark Dataframe- Handling Missing Values.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/this%20pc/Pictures/git%20repo/Pyspark-With-Python-main/Pyspark-With-Python-main/Tutorial%203-%20Pyspark%20Dataframe-%20Handling%20Missing%20Values.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature\u001b[39;00m \u001b[39mimport\u001b[39;00m Imputer\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/this%20pc/Pictures/git%20repo/Pyspark-With-Python-main/Pyspark-With-Python-main/Tutorial%203-%20Pyspark%20Dataframe-%20Handling%20Missing%20Values.ipynb#X44sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m imputer \u001b[39m=\u001b[39m Imputer(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/this%20pc/Pictures/git%20repo/Pyspark-With-Python-main/Pyspark-With-Python-main/Tutorial%203-%20Pyspark%20Dataframe-%20Handling%20Missing%20Values.ipynb#X44sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     inputCols\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mage\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mExperience\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSalary\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/this%20pc/Pictures/git%20repo/Pyspark-With-Python-main/Pyspark-With-Python-main/Tutorial%203-%20Pyspark%20Dataframe-%20Handling%20Missing%20Values.ipynb#X44sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     outputCols\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_imputed\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(c) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mage\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mExperience\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSalary\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/this%20pc/Pictures/git%20repo/Pyspark-With-Python-main/Pyspark-With-Python-main/Tutorial%203-%20Pyspark%20Dataframe-%20Handling%20Missing%20Values.ipynb#X44sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     )\u001b[39m.\u001b[39msetStrategy(\u001b[39m\"\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\this pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\__init__.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# Licensed to the Apache Software Foundation (ASF) under one or more\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# contributor license agreements.  See the NOTICE file distributed with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39mDataFrame-based machine learning APIs to let users quickly assemble and configure practical\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mmachine learning pipelines.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     Estimator,\n\u001b[0;32m     24\u001b[0m     Model,\n\u001b[0;32m     25\u001b[0m     Predictor,\n\u001b[0;32m     26\u001b[0m     PredictionModel,\n\u001b[0;32m     27\u001b[0m     Transformer,\n\u001b[0;32m     28\u001b[0m     UnaryTransformer,\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpipeline\u001b[39;00m \u001b[39mimport\u001b[39;00m Pipeline, PipelineModel\n\u001b[0;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     classification,\n\u001b[0;32m     33\u001b[0m     clustering,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     param,\n\u001b[0;32m     45\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\this pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\base.py:40\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     24\u001b[0m     Any,\n\u001b[0;32m     25\u001b[0m     Callable,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     TYPE_CHECKING,\n\u001b[0;32m     37\u001b[0m )\n\u001b[0;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m \u001b[39mimport\u001b[39;00m since\n\u001b[1;32m---> 40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparam\u001b[39;00m \u001b[39mimport\u001b[39;00m P\n\u001b[0;32m     41\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m \u001b[39mimport\u001b[39;00m inherit_doc\n\u001b[0;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparam\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mshared\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     43\u001b[0m     HasInputCol,\n\u001b[0;32m     44\u001b[0m     HasOutputCol,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m     Params,\n\u001b[0;32m     49\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\this pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\param\\__init__.py:32\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcopy\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     21\u001b[0m     Any,\n\u001b[0;32m     22\u001b[0m     Callable,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     TYPE_CHECKING,\n\u001b[0;32m     30\u001b[0m )\n\u001b[1;32m---> 32\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpy4j\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mjava_gateway\u001b[39;00m \u001b[39mimport\u001b[39;00m JavaObject\n\u001b[0;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinalg\u001b[39;00m \u001b[39mimport\u001b[39;00m DenseVector, Vector, Matrix\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=['age', 'Experience', 'Salary'], \n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['age', 'Experience', 'Salary']]\n",
    "    ).setStrategy(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e31190f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\this pc\\Pictures\\git repo\\Pyspark-With-Python-main\\Pyspark-With-Python-main\\Tutorial 3- Pyspark Dataframe- Handling Missing Values.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/this%20pc/Pictures/git%20repo/Pyspark-With-Python-main/Pyspark-With-Python-main/Tutorial%203-%20Pyspark%20Dataframe-%20Handling%20Missing%20Values.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature\u001b[39;00m \u001b[39mimport\u001b[39;00m Imputer\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/this%20pc/Pictures/git%20repo/Pyspark-With-Python-main/Pyspark-With-Python-main/Tutorial%203-%20Pyspark%20Dataframe-%20Handling%20Missing%20Values.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m imputer \u001b[39m=\u001b[39m Imputer(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/this%20pc/Pictures/git%20repo/Pyspark-With-Python-main/Pyspark-With-Python-main/Tutorial%203-%20Pyspark%20Dataframe-%20Handling%20Missing%20Values.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     inputCols\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mage\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mExperience\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSalary\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/this%20pc/Pictures/git%20repo/Pyspark-With-Python-main/Pyspark-With-Python-main/Tutorial%203-%20Pyspark%20Dataframe-%20Handling%20Missing%20Values.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     outputCols\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_imputed\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(c) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mage\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mExperience\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSalary\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/this%20pc/Pictures/git%20repo/Pyspark-With-Python-main/Pyspark-With-Python-main/Tutorial%203-%20Pyspark%20Dataframe-%20Handling%20Missing%20Values.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     )\u001b[39m.\u001b[39msetStrategy(\u001b[39m\"\u001b[39m\u001b[39mmedian\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\this pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\__init__.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# Licensed to the Apache Software Foundation (ASF) under one or more\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# contributor license agreements.  See the NOTICE file distributed with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39mDataFrame-based machine learning APIs to let users quickly assemble and configure practical\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mmachine learning pipelines.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     Estimator,\n\u001b[0;32m     24\u001b[0m     Model,\n\u001b[0;32m     25\u001b[0m     Predictor,\n\u001b[0;32m     26\u001b[0m     PredictionModel,\n\u001b[0;32m     27\u001b[0m     Transformer,\n\u001b[0;32m     28\u001b[0m     UnaryTransformer,\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpipeline\u001b[39;00m \u001b[39mimport\u001b[39;00m Pipeline, PipelineModel\n\u001b[0;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     classification,\n\u001b[0;32m     33\u001b[0m     clustering,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     param,\n\u001b[0;32m     45\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\this pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\base.py:40\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     24\u001b[0m     Any,\n\u001b[0;32m     25\u001b[0m     Callable,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     TYPE_CHECKING,\n\u001b[0;32m     37\u001b[0m )\n\u001b[0;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m \u001b[39mimport\u001b[39;00m since\n\u001b[1;32m---> 40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparam\u001b[39;00m \u001b[39mimport\u001b[39;00m P\n\u001b[0;32m     41\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m \u001b[39mimport\u001b[39;00m inherit_doc\n\u001b[0;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparam\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mshared\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     43\u001b[0m     HasInputCol,\n\u001b[0;32m     44\u001b[0m     HasOutputCol,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m     Params,\n\u001b[0;32m     49\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\this pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\param\\__init__.py:32\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcopy\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     21\u001b[0m     Any,\n\u001b[0;32m     22\u001b[0m     Callable,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     TYPE_CHECKING,\n\u001b[0;32m     30\u001b[0m )\n\u001b[1;32m---> 32\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpy4j\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mjava_gateway\u001b[39;00m \u001b[39mimport\u001b[39;00m JavaObject\n\u001b[0;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinalg\u001b[39;00m \u001b[39mimport\u001b[39;00m DenseVector, Vector, Matrix\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=['age', 'Experience', 'Salary'], \n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['age', 'Experience', 'Salary']]\n",
    "    ).setStrategy(\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d84c4a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "|     Name| age|Experience|Salary|age_imputed|Experience_imputed|Salary_imputed|\n",
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "|    Krish|  31|        10| 30000|         31|                10|         30000|\n",
      "|Sudhanshu|  30|         8| 25000|         30|                 8|         25000|\n",
      "|    Sunny|  29|         4| 20000|         29|                 4|         20000|\n",
      "|     Paul|  24|         3| 20000|         24|                 3|         20000|\n",
      "|   Harsha|  21|         1| 15000|         21|                 1|         15000|\n",
      "|  Shubham|  23|         2| 18000|         23|                 2|         18000|\n",
      "|   Mahesh|null|      null| 40000|         29|                 4|         40000|\n",
      "|     null|  34|        10| 38000|         34|                10|         38000|\n",
      "|     null|  36|      null|  null|         36|                 4|         20000|\n",
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add imputation cols to df\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd38651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63a65ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2992b6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d995bc12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17fd913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf76805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835812ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
