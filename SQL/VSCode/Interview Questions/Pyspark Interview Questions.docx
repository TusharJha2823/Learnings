1. Can you explain the differences between a DataFrame and an RDD in PySpark?
Answer :- 
    Structure - RDDs are a collection of data objects across nodes in a cluster, while DataFrames are similar to database tables with rows and columns.
    Level of abstraction - RDDs are a fundamental data structure that offers low-level operations, while DataFrames are higher-level abstractions built on top of RDDs
    Optimization - RDDs lack optimization benefits, while DataFrames offer optimizations for better performance.
    APIs - RDDs provide low-level APIs for processing distributed data, while DataFrames provide higher-level APIs that support SQL methods. 
    Flexibility - RDDs offer flexibility but with less optimization and type safety.

2. What techniques would you use to optimize the performance of PySpark code?
Answer :- 
    Data partitioning - Divide large datasets into smaller, balanced chunks to minimize network traffic and data shuffling. Partitioning by frequently used columns can improve filtering and aggregation. 
    Caching - Use cache and persist functions to reduce data frame processing time. However, be mindful of memory overhead and leave room for other Spark tasks. 
    Joins - Use broadcast joins when one DataFrame is much smaller to reduce data movement. Choose appropriate join types like hash joins for large tables and broadcast joins for small tables. 
    Parallelism - Increase the number of Spark partitions based on data size to optimize parallelism and resource utilization. 
    UDFs - Avoid unnecessary UDFs and optimize existing ones to reduce serialization and deserialization. 
    DataFrames API - Use the Spark DataFrame API instead of RDDs as it provides optimized execution plans and automatic optimization for many data processing tasks. 
    Serialization - Minimize the use of closures and avoid unnecessary object creation to optimize data processing logic and reduce serialization and deserialization operations. 
    Spark UI- Monitor PySpark jobs using the Spark UI for detailed information on job stages, tasks, and resource usage. 
    Cluster configuration - Set up the Spark cluster to run efficiently. 
    Avoid .collect() - Using df.collect() returns all results to the driver node, which can lead to out of memory issues with large data sets. Use df.take(5) to observe the data instead. 
    Minimize data shuffling - Carefully manage data movements within Spark DataFrames and optimize operations to reduce shuffling.


3. How does the Catalyst Optimizer contribute to query execution in PySpark?
Answer :- 
    The Catalyst Optimizer in PySpark significantly enhances query execution by performing logical and physical optimizations. It analyzes queries and generates an optimized execution plan, applying techniques like predicate pushdown, column pruning, and constant folding. Catalyst also optimizes join strategies and leverages code generation to produce efficient bytecode. These optimizations result in faster query execution, reduced data processing, and improved overall performance of PySpark applications.

4. Which serialization formats are commonly used in PySpark, and why?
Answer :- 
    PySpark commonly uses three serialization formats: Java serialization (default), Kryo serialization, and Apache Avro. Java serialization is the default but can be slow for large datasets. Kryo serialization is faster and more compact, making it preferred for performance-critical applications. Apache Avro is schema-based, supports schema evolution, and is often used for data storage and interoperability between Hadoop ecosystems. The choice depends on factors like performance requirements, data complexity, and compatibility needs.

5. How do you address skewed data issues in PySpark?
Answer :- 
    Repartition: Use the repartition() or repartitionByRange() functions to evenly distribute data across partitions and balance workload. You can increase or decrease the number of partitions in a DataFrame or RDD. 
    Filter and split: Identify the keys causing skew and process the skewed and non-skewed data separately. This method is useful when a small subset of keys disproportionately affects join performance. 
    Salting: Add a random or pseudo-random value to the data to spread it more evenly across partitions. This technique is useful when certain keys in your data have a high number of occurrences. 
    Broadcast join: Use the BROADCAST hint to avoid skewness. 
    Adaptive Query Execution (AQE): Available in Spark 3.0 and newer, AQE uses statistics to dynamically adjust the execution plan based on runtime statistics. 
    Log transformation: Compress large values and expand small values to reduce right-skewness. 

6. Could you describe how memory management is handled in PySpark?
Answer :- 
    Executor Memory: Each executor in a Spark cluster is allocated a certain amount of memory. This memory is divided into two main parts:
    Storage Memory: Used for caching RDDs and DataFrames in memory.
    Execution Memory: Used for computations and storing intermediate results during transformations.
    Unified Memory Management: Spark employs a unified memory management model, which allows dynamic allocation between storage and execution memory. This means if the storage memory is full, Spark can borrow memory from the execution pool, and vice versa.
    Storage Levels: PySpark provides different storage levels that determine how data is persisted in memory or on disk:
        MEMORY_ONLY: Store data only in memory.
        MEMORY_AND_DISK: Store data in memory, spilling to disk if memory is full.
        DISK_ONLY: Store data only on disk.
    Garbage Collection: PySpark relies on the Java garbage collector to reclaim memory from objects that are no longer referenced.

    Important Considerations: 
        Data Serialization: PySpark serializes data before sending it to executors, which can impact memory usage. Consider using efficient serialization formats like Kryo.
        Caching: Caching RDDs and DataFrames can significantly improve performance, but it's important to cache only the necessary data to avoid memory issues.
        Broadcast Variables: Broadcast variables are distributed to all executors once, reducing the overhead of sending the same data to each task.
        Memory Tuning: You can tune Spark's memory management parameters, such as spark.memory.fraction and spark.memory.storageFraction, to optimize performance based on your application's needs.

    Monitoring Memory Usage:
        Spark UI: The Spark UI provides insights into memory usage on the cluster, including storage and execution memory usage.
        PySpark Profilers: Profiling tools like the pyspark.profiler module can help identify memory bottlenecks in your PySpark code.

7. What are the different types of join strategies in PySpark, and how do you implement them?
Answer :- 
    PySpark offers a number of join strategies, including Shuffle Hash Join, Broadcast Hash Join, and Sort Merge Join: 
    Shuffle Hash Join - Shuffles both datasets to ensure matching keys are in the same partition, then hashes the smaller dataset into buckets. 
    Broadcast Hash Join - Also known as a map-side join, this strategy sends a copy of one join relation to all worker nodes, saving shuffle costs. It's ideal when joining a large dataset with a smaller one. 
    Shuffle Sort Merge Join - Spark's default join strategy, this method involves shuffling data to group matching join keys with the same worker. Then, a sort-merge join is performed at the partition level on the worker nodes. 
    Other join strategies include: 
        Cartesian Product Join: A join strategy for non-equi joins that involve comparisons to unspecified values. 
        Broadcast Nested Loop Join: Another join strategy for non-equi joins. 
    
    Spark configuration settings like spark.sql.shuffle.partitions and spark.default.parallelism can be used to fine-tune data partitioning in a PySpark application.

8. What is the purpose of the `broadcast()` function in PySpark, and when should it be used?
Answer :- 
    In PySpark, the broadcast() function explicitly tells a DataFrame to be broadcasted during a join. Broadcasting is a programming mechanism in Apache Spark that keeps a read-only copy of a DataFrame's data on each node in a cluster. It's used to distribute small, read-only datasets across all tasks within a Spark job. Broadcasting can improve performance and memory efficiency. 

Here are some situations when broadcasting is useful: 
    Join operations - Broadcasting a smaller DataFrame or RDD can improve performance when joining large and small datasets. 
    Lookup tables - Broadcasting lookup tables to worker nodes can avoid shuffling the entire dataset across the network during join operations. 
    Configuration parameters - Broadcasting configuration parameters ensures they are available locally on each worker node. 
    Trained models - Broadcasting a trained model allows each machine to access it locally during the prediction phase. 
    Broadcasting is suitable for small datasets. For two large datasets, a normal join is more suitable.

9. How do you define and use User-Defined Functions (UDFs) in PySpark?
Answer :- 
    In PySpark, User-Defined Functions (UDFs) allow you to extend the functionality of Spark SQL by incorporating your own custom Python functions. Here's how you can define and use UDFs:
    1. Define the Python Function:
        Create a regular Python function that performs the desired operation.
        def square(x):
            return x ** 2

    2. Convert to PySpark UDF:
        Use the udf function from the pyspark.sql.functions module to register your Python function as a UDF. You also need to specify the return data type.
        
        from pyspark.sql.functions import udf, IntegerType
        square_udf = udf(square, IntegerType())

    3. Apply the UDF to a DataFrame:
        Use the registered UDF in your DataFrame transformations.
        
        from pyspark.sql import SparkSession
        spark = SparkSession.builder.getOrCreate()
        df = spark.createDataFrame([(1,), (2,), (3,)], ["value"])
        df = df.withColumn("squared_value", square_udf("value"))
        df.show()
    
    Important Considerations:
        Data Types: Ensure the data types of your UDF's input and output match the data types in your DataFrame.
        Performance: While UDFs provide flexibility, they can be less performant than native Spark SQL functions.
        Null Handling: Consider how your UDF handles null values.
        Pandas UDFs: For improved performance, consider using Pandas UDFs, which operate on batches of data.

10. What is lazy evaluation in PySpark, and how does it affect job execution?
Answer :- 
    Lazy evaluation in PySpark is a strategy that delays executing transformations on DataFrames or RDDs until an action is triggered. Instead, Spark builds a directed acyclic graph (DAG) to represent the computation and only executes it when an action is triggered. This technique can optimize job execution by: 
    
    Pipelining transformations - Spark can rearrange and combine transformations to optimize the execution plan. 
    Avoiding unnecessary computations - Spark can choose the most efficient join strategies based on the actual data distribution. For example, if the requested results don't require reading the entire file, Spark will only read from the first partition. 
    Reducing data shuffling - Lazy evaluation can reduce the amount of data shuffled and the number of stages. 
    Preventing memory errors - Lazy evaluation can help prevent memory errors. 

11. What are the steps to create a DataFrame in PySpark?
Answer :- 
    To create a DataFrame in PySpark, you can follow these steps:
    Step 1: Create a SparkSession
        from pyspark.sql import SparkSession
        spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()
    
    Step 2: Create the DataFrame
        You can create a DataFrame using various methods:
        From a list of lists or tuples:
            data = [("Alice", 25), ("Bob", 30)]
            df = spark.createDataFrame(data, ["name", "age"])

        From an RDD.
            rdd = spark.sparkContext.parallelize(data)
            df = rdd.toDF(["name", "age"])
        
        From a CSV file.
            df = spark.read.csv("path/to/file.csv", header=True, inferSchema=True)
        
        From a JSON file.
            df = spark.read.json("path/to/file.json")
        
        From a Parquet file.
            df = spark.read.parquet("path/to/file.parquet")
        
    Step 3: View the DataFrame
        df.show()

12. Could you explain the concept of Resilient Distributed Datasets (RDD) in PySpark?
Answer :- 
    A Resilient Distributed Dataset (RDD) is a collection of data elements that are partitioned across nodes in a cluster, and are the primary user-facing API in Apache Spark. RDDs are immutable, meaning they can't be changed after they are created, but they are fault-tolerant and will recover from failures. 
    
    Here are some things to know about RDDs: 
    Parallel processing - RDDs can be operated in parallel, which allows for faster processing of large data sets. 
    Fault tolerance - RDDs can recover from failures through lineage reconstruction. 
    Partitioning- Apache Spark automatically partitions RDDs and distributes them across nodes. 
    Creation - RDDs can be created by parallelizing an existing collection or referencing a dataset in an external storage system. 
    Operations - RDDs can be used for a variety of operations, including filter(), flatMap(), and coalesce()

13. What are actions and transformations in PySpark, and how do they differ?
Answer :- 
    In PySpark, both transformations and actions are fundamental operations that can be performed on DataFrames, Datasets, and Resilient Distributed Datasets (RDDs). However, they differ in their purpose and execution: 
    
    Transformations -Shape data by creating new RDDs. They are lazy, meaning they aren't immediately executed. Examples include filter, map, and groupBy. 
    Actions - Trigger the execution of transformations and return a value to the driver program or write data to external storage. Examples include take, reduce, and collect. 

    Here's a deeper dive into their differences: 
        Execution: Transformations are lazy, so they aren't executed until an action is called on the Spark RDD. 
        Results: Transformations shape the data, while actions compute the results. 
        Data manipulation: Transformations are operations that change one DataFrame or RDD into another, leaving the original one unchanged. 
        External storage: Actions can write data to external storage systems. 
        Lineage: Transformations create a lineage since DataFrame and RDD objects are immutable. 

14. How do you manage and handle null values in PySpark DataFrames?
Answer :- 
    Managing and handling null values in PySpark DataFrames is a crucial part of data cleaning and preparation. Here's how you can do it:
    
    Identifying Null Values
    isNull(): This function can be used to create a boolean column indicating whether a value is null.
        
        from pyspark.sql.functions import isnull, col
        df.withColumn("is_age_null", isnull(col("age"))).show()

    isNotNull(): This function does the opposite, creating a boolean column indicating whether a value is not null.
        df.withColumn("is_age_not_null", isnull(col("age"))).show()
    
    df.filter(): To filter rows with null values in a specific column:
        df.filter(col("age").isNull()).show()

    Handling Null Values
    Dropping Nulls:
        df.na.drop(): Drops rows containing null values.
           df.na.drop().show() 
        
        df.na.drop(how="all"): Drops rows where all columns are null.
           df.na.drop(how="all").show() 

        df.na.drop(subset=["age", "city"]): Drops rows where null values exist in specific columns.
           df.na.drop(subset=["age", "city"]).show()

    Filling Nulls:
        df.na.fill(value): Fills null values with a specified value.
           df.na.fill(0, subset=["age"]).show()
        
        df.na.fill({"age": 0, "city": "Unknown"}): Fills null values with different values for different columns.
           df.na.fill({"age": 0, "city": "Unknown"}).show()

    Choosing the Right Approach
    Dropping: If the dataset is large and null values are few, dropping might be suitable.
    Filling: If you have a good understanding of the data and can provide a meaningful replacement value, filling is an option.
    Imputing: If you want to use statistical methods for replacement, imputing is a good approach.

15. What is a partition in PySpark, and how do you control partitioning for better performance?
16. Can you explain the difference between narrow and wide transformations in PySpark?
17. How does PySpark infer schemas, and what are the implications of this?
18. What role does SparkContext play in a PySpark application?
19. How do you perform aggregations in PySpark, and what are the key considerations?
20. What strategies do you use for caching data in PySpark to improve performance?
21. What is ETL (Extract, Transform, Load)?
22. What is a Data Warehouse?
23. What is a Data Lake?
24. What is the difference between a Data Warehouse and a Data Lake?
25. What is batch processing?
26. What is stream processing?
27. What is the difference between batch processing and stream processing?
28. What is a Lambda Architecture?
29. What are the components of a Lambda Architecture?
30. What is a Kappa Architecture?
31. What is a scalable system?
32. How do you approach a system design interview?
33. What is the difference between horizontal and vertical scaling?
34. What are the advantages of horizontal scaling?
35. What is a load balancer?
36. How does a load balancer improve system reliability?
37. What is caching?
38. What are the types of caches?
39. What is database replication?
40. What are the benefits of database replication?
41. Explain how PySpark handles schema validation.
42. How can you broadcast variables in PySpark?
43. What are the different ways to run Spark jobs on a cluster?
44. Describe PySpark UDFs and their use cases.
45. How do you optimize a PySpark job?
46. Explain the concept of lineage in PySpark.
47. What are accumulators, and how do you use them?
48. How do you manage memory in PySpark?
49. What is the significance of the DAG (Directed Acyclic Graph) in Spark?
50. Explain how to perform data partitioning in PySpark.
51. How do you handle skewed data in PySpark?
52. What is a PySpark RDD, and how does it differ from DataFrames?
53. Describe the PySpark Catalyst Optimizer.
54. How do you perform joins in PySpark, and what types are available?
55. What is the role of the SparkContext in PySpark?
56. How do you handle missing or null values in PySpark?
57. What is the difference between partitioning and Bucketing?
58. What is the advantage and disadvantage of UDF in Spark/PySpark?
59. How can you persist and cache data in PySpark?
60. What is the speculative execution?
61. Difference between Static Partitioning and Dynamic Partitioning ?
62. How do you handle job failures in an ETL pipeline?
63. What steps do you take when a data pipeline is running slower than expected?
64. How do you address data quality issues in a large dataset?
65. What would you do if a scheduled job didn't trigger as expected?
66. How do you troubleshoot memory-related issues in Spark jobs?
67. What is your approach to handling schema changes in source systems?
68. How do you manage data partitioning in large-scale data processing?
69. What do you do if data ingestion from a third-party API fails?
70. How do you resolve issues with data consistency between different data stores?
71. How do you handle out-of-memory errors in a Spark job?
72. What steps do you take when a data job exceeds its allocated time window?
73. How do you manage and monitor data pipeline dependencies?
74. What do you do if the output of a data transformation step is incorrect?
75. How do you address issues with data duplication in a pipeline?
76. How do you handle and log errors in a distributed data processing job?
77. Can you explain your project flow and architecture?
78. What is the default file format used in Spark?
79. Why is Parquet commonly used in Spark?
80. What optimization techniques have you implemented in your projects?
81. Can you explain the difference between 'groupByKey' and 'reduceByKey' in Spark? Which one is more efficient?
82. What do you understand by rack awareness in Hadoop?
83. What file formats do you typically use in your data processing workflows?
84. How does fault tolerance work in Spark?
85. How would you handle and ignore null values while loading data?
86. How would you find the 3rd highest salary in a dataset?
87. Given a dataset with positive and negative invoice values, how would you convert the positive values to negative while keeping the negative values unchanged?
88. How can you convert a date like "20/04/1963" to an integer format?
89. Given a dataset containing alphanumeric values and integers, how would you extract specific alphanumeric sequences like "ML," "GM," and "LTR" and create a new DataFrame to view only these sequences in Spark?
90. What kind of questions have you encountered related to data modeling in your projects?

Delta li
asdFaerwt3aekERAWTve tables 

91. What are Delta Live Tables and how do they work?

92. How can Delta Live Tables improve data pipeline development and management?

93. Describe the process of building and deploying a data pipeline using Delta Live Tables.

94. What are the benefits of using Delta Live Tables in a data lakehouse?

95. How can Delta Live Tables ensure data quality and reliability in real-time analytics?

96. What are the challenges of parsing unstructured data such as images and textual data?

97. What are the key components of a data lakehouse architecture?

98. How do you ensure data quality and consistency in a real-time analytics system?

99. How do you handle large volumes of event data in a lakehouse?

100. How can you handle incomplete or poor-quality data in regulatory reporting?



Azure Databricks

101. What are the key features of Azure Databricks?

102. Which cloud services include Azure Databricks?

103. What programming languages does Azure Databricks support?

104. Can you explain the management plane in Azure Databricks?

105. What are the benefits of using Microsoft Azure Databricks?

106. What pricing models are available for Azure Databricks?

107. What is a Databricks Unit (DBU) in Azure Databricks?

108. Can you describe the DBU Framework in Azure Databricks?

109. What is a DataFrame in Azure Databricks?

110. What is caching, and what are its types?

111. What are clusters and instances in Azure Databricks?

112. What is a Delta Lake Table?

113. How are widgets used in Azure Databricks?

114. What challenges might you face with Azure Databricks?

115. What is the control plane in Azure Databricks?

116. What are collaborative workspaces in Azure Databricks?

117. What is serverless database processing in Azure Databricks?

118. How is Kafka used in Azure Databricks?

119. How do you process large datasets in Azure Databricks?

120. How do you troubleshoot issues in Azure Databricks?

121. How do you secure sensitive data in Azure Databricks?




122. How do you deploy PySpark applications in a production environment?

123. What are some best practices for monitoring and logging PySpark jobs?

124. How do you manage resources and scheduling in a PySpark application?

125. Write a PySpark job to perform a specific data processing task (e.g., filtering data, aggregating results).

126. You have a dataset containing user activity logs with missing values and inconsistent data types. Describe how you would clean and standardize this dataset using PySpark.

127. Given a dataset with nested JSON structures, how would you flatten it into a tabular format using PySpark?

128. Your PySpark job is running slower than expected due to data skew. Explain how you would identify and address this issue.

129. You need to join two large datasets, but the join operation is causing out-of-memory errors. What strategies would you use to optimize this join?

130. Describe how you would set up a real-time data pipeline using PySpark and Kafka to process streaming data.

131. You are tasked with processing real-time sensor data to detect anomalies. Explain the steps you would take to implement this using PySpark

132. Describe how you would design and implement an ETL pipeline in PySpark to extract data from an RDBMS, transform it, and load it into a data warehouse.

133. Given a requirement to process and transform data from multiple sources (e.g., CSV, JSON, and Parquet files), how would you handle this in a PySpark job?

134. You need to integrate data from an external API into your PySpark pipeline. Explain how you would achieve this.

135. Describe how you would use PySpark to join data from a Hive table and a Kafka stream.

136. You need to integrate data from an external API into your PySpark pipeline. Explain how you would achieve this.


137. Can you explain the differences between a DataFrame and an RDD in PySpark?

138. What techniques would you use to optimize the performance of PySpark code?

139. How does the Catalyst Optimizer contribute to query execution in PySpark?

140. Which serialization formats are commonly used in PySpark, and why?

141. How do you address skewed data issues in PySpark?

142. Could you describe how memory management is handled in PySpark?

143. What are the different types of joins in PySpark, and how do you implement them?

144. What is the purpose of the 'broadcast() function in PySpark, and when should it be used?

145. How do you define and use User-Defined Functions (UDFs) in PySpark?

146. What is lazy evaluation in PySpark, and how does it affect job execution?

147. What are the steps to create a DataFrame in PySpark?

148. Could you explain the concept of Resilient Distributed Datasets (RDD) in PySpark?

149. What are actions and transformations in PySpark, and how do they differ?

150. How do you manage and handle null values in PySpark DataFrames?

151. What are the different spark.read formats?

152.For a 16 node cluster with 16 cores each calculate the number of executors

153. where do you use repartition and coalesce. give few scenarios.

154. write the dataframe to a delta table specifying catalog, schema and table name

155.How do you use unity catalog for data governance in your project

156.Define ACID properties and cap theorem

157.What is data modeling

158.what is fact and dimension tables?

159.Mention some dbutils functions ad its uses

160.what is CDC(change data capture)?









Azure Data Factory (ADF)
Easy Questions

161. What is Azure Data Factory used for?
162. Explain the concept of a pipeline in ADF.
163. What is a linked service in ADF?
164. How do you schedule a pipeline in ADF?
165. What is a dataset in the context of ADF?

Tough Questions
166. Explain the differences between Mapping Data Flows and Wrangling Data Flows in ADF.
167. How would you implement dynamic pipeline execution in ADF?
168. Describe the process of implementing incremental load patterns using ADF.
169. How do you handle error logging and notifications in ADF pipelines?
170. Explain the concept of integration runtime in ADF and its types.

PySpark
Easy Questions

171. What is PySpark?
172. How do you create a SparkSession in PySpark?
173. Explain the difference between an RDD and a DataFrame in PySpark.
174. What is a transformation in PySpark? Give an example.
175. How do you read CSV file in PySpark?

Databricks

176. Explain the architecture of Delta Lake in Databricks and its benefits.
177. How would you implement multi-hop architecture in Databricks?
178. Describe the process of implementing auto-scaling in Databricks clusters.
179. How do you optimize Databricks Delta table performance?
180. Explain the concept of Databricks Unity Catalog and how it enhances data governance.

Dimensional Modeling
Tough Questions

181. Describe the process of designing a slowly changing dimension (SCD) Type 2.
182. How would you handle a many-to-many relationship in dimensional modeling?
183. Explain the concept of a junk dimension and when you would use it.
184. Describe the pros and cons of using a snowflake schema versus a star schema.
185. How would you design a dimension to handle hierarchical data?

186. What is a Star Schema and how does it differ from a Snowflake Schema?
Answer :- 
    A Star Schema is a data warehouse schema that consists of a central fact table surrounded by dimension tables. A Snowflake Schema is a more normalized version of a Star Schema, where each dimension table is further divided into multiple related tables.

187. What is the difference between a Fact Table and a Dimension Table?
Answer :- 
    A Fact Table stores measurable data, such as sales or revenue, whereas a Dimension Table stores descriptive data, such as date, time, or geography.

188. What is Data Aggregation and how is it used in a Data Warehouse?
Answer :- 
    Data Aggregation is the process of grouping and summarizing data to provide a higher level of detail. In a Data Warehouse, aggregation is used to improve query performance and provide faster access to data.

189. What is Data Partitioning and why is it used in a Data Warehouse?
Answer :- 
    Data Partitioning is the process of dividing large tables into smaller, more manageable pieces. It is used in a Data Warehouse to improve query performance, reduce data loading times, and make maintenance easier.

190. What is the difference between a Type 1 and Type 2 Slowly Changing Dimension (SCD)?
Answer :- 
    A Type 1 SCD overwrites old data with new data, while a Type 2 SCD creates a new record for each change, preserving historical data.

191. What is the difference between SQL normal Views vs. Materialized Views?
Answer :- 
    SQL Views are virtual tables derived from SQL queries and do not store data physically. They fetch data dynamically from base tables, ensuring the most current data is always presented. This makes them ideal for simplifying complex queries, providing data abstraction, and enhancing security by restricting access to specific data.
    
    Materialized Views, on the other hand, store the query results physically, offering precomputed data for faster read operations. They require periodic refreshes to ensure data accuracy. These views are particularly useful for improving performance in complex queries and storing aggregated data for quick access.

192. How can I address data skewness using repartitioning?
Answer:- 
    Repartition the DataFrame by a column with high cardinality to ensure more balanced data distribution.
    PySpark Code - df = df.repartition("columnName") # Use a column with High cardinality

193. What is the process for salting to mitigate data skewness?
Answer:- 
    Add a salt function to distribute the data more evenly and create a new key.
    
    PySpark Code - 
    from pyspark.sql.functions import col, expr
    df = df.withColumn("salt", expr("floor(rand() * num_salts)"))
    df = df.withColumn("new_key", col("columnName") + "_" + col("salt"))


194. How can I optimize joins to reduce data skewness?
Answer:- 
    Use broadcast for smaller tables to join with high cardinality performance.
    from pyspark.sql.functions import broadcast
    PySpark Code - df = df.join(broadcast(small_df), "join_column")


195. Explain the difference between Caching and Persisting in Spark?
Answer:- 
    𝗖𝗮𝗰𝗵𝗶𝗻𝗴: Caching in Apache Spark involves storing RDDs in memory temporarily. When an RDD is cached, its partitions are kept in memory across multiple operations, allowing for faster access and reuse of intermediate results.

    𝗣𝗲𝗿𝘀𝗶𝘀𝘁𝗶𝗻𝗴: Persisting in Apache Spark is similar to caching but offers more flexibility in terms of storage options. When you persist an RDD, you can specify different storage levels such as MEMORY_ONLY, MEMORY_AND_DISK, or DISK_ONLY, depending on your requirements

➤ 𝗞𝗲𝘆 𝗱𝗶𝗳𝗳𝗲𝗿𝗲𝗻𝗰𝗲𝘀 𝗯𝗲𝘁𝘄𝗲𝗲𝗻 𝗰𝗮𝗰𝗵𝗶𝗻𝗴 𝗮𝗻𝗱 𝗽𝗲𝗿𝘀𝗶𝘀𝘁𝗶𝗻𝗴:

    - While caching stores RDDs in memory by default, persisting allows you to choose different storage levels, including disk storage. Caching is suitable for scenarios where RDDs need to be reused in subsequent operations within the same Spark job.
    - whereas persisting is more versatile and can be used to store RDDs across multiple jobs or even persist them to disk for fault tolerance.

➤ 𝗘𝘅𝗮𝗺𝗽𝗹𝗲 𝗼𝗳 𝘄𝗵𝗲𝗻 𝘆𝗼𝘂 𝘄𝗼𝘂𝗹𝗱 𝘂𝘀𝗲 𝗰𝗮𝗰𝗵𝗶𝗻𝗴 𝘃𝗲𝗿𝘀𝘂𝘀 𝗽𝗲𝗿𝘀𝗶𝘀𝘁𝗶𝗻𝗴

    - Let's say we have an iterative algorithm where the same RDD is accessed multiple times within a loop. In this case, caching the RDD would be beneficial as it would avoid recomputation of the RDD's partitions in each iteration, resulting in significant performance gains. 
    - On the other hand, if we need to persist RDDs across multiple Spark jobs or need fault tolerance, persisting would be more appropriate.

➤ 𝗛𝗼𝘄 𝗱𝗼𝗲𝘀 𝗦𝗽𝗮𝗿𝗸 𝗵𝗮𝗻𝗱𝗹𝗲 𝗰𝗮𝗰𝗵𝗶𝗻𝗴 𝗮𝗻𝗱 𝗽𝗲𝗿𝘀𝗶𝘀𝘁𝗶𝗻𝗴 𝘂𝗻𝗱𝗲𝗿 𝘁𝗵𝗲 𝗵𝗼𝗼𝗱

    - Spark employs a lazy evaluation strategy, so RDDs are not actually cached or persisted until an action is triggered. When an action is called on a cached or persisted RDD, Spark checks if the data is already in memory or on disk. If not, it calculates the RDD's partitions and stores them accordingly based on the specified storage level.

196. Explain Spark Architecture.

197. What is Z-ordering?

198. Difference between Datamart, Datawarehouse and Deltalake

199. Compare performance of Managed and External Table

200. Difference between blob Store and ADLS Gen2    


_________________________________________________________________________________________________________

KPMG / EY / PwC / Deloitte Consulting PySpark interview questions for Data Engineers

201. How do you optimize the performance of your PySpark jobs?
202. Can you discuss the techniques you use to handle skewed data in PySpark?
203. Can you describe how you’ve applied Spark optimization techniques in your previous roles?
204. Could you provide an overview of your experience with PySpark and big data processing?
205. Can you explain the basic architecture of PySpark in detail?
206. What are the differences between a DataFrame and an RDD in PySpark, and in what scenarios would you use an RDD?
207. How does PySpark relate to Apache Spark, and what advantages does it offer for distributed data processing?
208. Can you explain the concepts of transformations and actions in PySpark?
209. Could you provide examples of PySpark DataFrame operations that you frequently use?
210. How does data serialization work in PySpark?
211. Can you discuss the significance of choosing the right compression codec for your PySpark applications?
212. How do you handle missing or null values in PySpark?
213. Are there any specific strategies or functions you prefer for handling missing data in PySpark?
214. What are some common challenges you’ve encountered while working with PySpark, and how have you overcome them?
215. How do you ensure data quality and integrity when processing large datasets with PySpark?
216. Can you share an example of a complex PySpark project you’ve worked on and the results you achieved?

_________________________________________________________________________________________________________
Deloitte PySpark interview questions for Data Engineer 2024

217. Can you provide an overview of your experience working with PySpark and big data processing?
218. What motivated you to specialize in PySpark, and how have you applied it in your previous roles?
219. Explain the basic architecture of PySpark.
220. How does PySpark relate to Apache Spark, and what advantages does it offer in distributed data processing?
221. Describe the difference between a DataFrame and an RDD in PySpark.
222. Can you explain transformations and actions in PySpark DataFrames?
223. Provide examples of PySpark DataFrame operations you frequently use.
224. How do you optimize the performance of PySpark jobs?
225. Can you discuss techniques for handling skewed data in PySpark?
226. Explain how data serialization works in PySpark.
227. Discuss the significance of choosing the right compression codec for your PySpark applications.
228. How do you deal with missing or null values in PySpark DataFrames?
229. Are there any specific strategies or functions you prefer for handling missing data?
230. Describe your experience with PySpark SQL.
2️31. How do you execute SQL queries on PySpark DataFrames?
232. What is broadcasting, and how is it useful in PySpark?
233. Provide an example scenario where broadcasting can significantly improve performance.
234. Discuss your experience with PySpark’s MLlib.
235. Can you give examples of machine learning algorithms you’ve implemented using PySpark?
236. Describe the importance of logging in PySpark applications.
237. Have you integrated PySpark with other big data technologies or databases? If so, please provide examples.
238. How do you handle data transfer between PySpark and external systems?
239. Explain the project that you worked on in your previous organizations.
240. Describe a challenging PySpark project you’ve worked on. What were the key challenges, and how did you overcome them?
241. Explain your experience with cluster management in PySpark.
242. How do you scale PySpark applications in a cluster environment?
243. Can you name and briefly describe some popular libraries or tools in the PySpark ecosystem, apart from the core PySpark functionality?
244. What cluster Manager have you used in your project?
245. How does your data come to your storage location?
246. What is the cluster size?
247. What are the other sources you have used in your project?
2️48. What is the frequency of your data in your source?
249. What is the sink for the project?
2️50. What is the volume of your data?
251. Please explain your task details completed. Let’s say out of 100 tasks, 99 tasks are completed, however, the last task is taking long hours to finish/complete. How do you handle this issue?
252. What challenges have you faced and how did you overcome them?
253. What optimization technique have you used in your project and what is the reason for it? Have you done spark optimization tuning? If yes, how have you done that?
254. Can you please walk me through the spark-submit command?
255. Let’s say you are getting your data volume is 100 GB. In your spark, you are doing 5 actions and 3 transformations on the data. Explain what goes behind the scene with respect to stages and tasks.
256. How do you take your code to the higher environment?
257. How do you schedule your job in production?
258. How do you reprocess the data if it failed?
259. Tell me one scenario where you have gone wrong with your decision making and what you have learnt from that mistake.
260. Let’s say you have noticed duplicate records loaded in the table for a particular partition. How do you resolve such issues?
261. What is the frequency of your jobs?
262. How do you notify your business/stakeholders in case of any job failure?



Coding Questions: Pyspark:
You've been given some CSV files like karnataka.csv and maharashtra.csv in an ADLS location, each containing columns for first_name, last_name, age, sex, and location.
Your task is to add a new column called state to each DataFrame. The state column should contain the state name extracted from the filename.
For example:
For karnataka.csv, the state column should contain the value 'karnataka'.
For maharashtra.csv, the state column should contain the value 'maharashtra'.
Your solution should utilize PySpark to efficiently handle large-scale data processing tasks.

2. Coding Question: Python/ DSA:
A. Reverse a string without using reverse function
B. Write a python program to implement following. Given a pattern and a string. If string matches the pattern return true else return false. 
Ex: 
Pattern – “abba”, String – “dog cat cat dog” will return true. 
Pattern – “aba”, String – “dog dog cat” will return false.

3. Coding Question: SQL
Find the 3-month rolling average of total revenue from purchases given a table with users, their purchase amount, and date purchased (YYYY-MM-DD). Output the year-month (YYYY-MM) and 3-month rolling average of revenue, sorted from earliest month to latest month.




Fractal Interview Questions
The Problem: You are given a DataFrame df with student data, which includes their names and cumulative GPAs. The DataFrame schema includes columns like student_name and cumulative_gpa.
Tasks:
1.Find the maximum, minimum, and average cumulative GPA from the DataFrame.
2.Determine the student with the highest GPA and the student with the lowest GPA.
3.Count the number of students whose GPA matches the rounded average GPA.
4.Create a new DataFrame with a schema containing the names of the students with the highest and lowest GPAs, as well as the count of students with the average GPA.
5.Display this new DataFrame.

from pyspark.sql.functions import col, max, min, avg, round

# Finding the max, min, and average GPA
max_gpa = df.select(max(col("cumulative_gpa"))).collect()[0][0]
min_gpa = df.select(min(col("cumulative_gpa"))).collect()[0][0]
avg_gpa = df.select(round(avg(col("cumulative_gpa")), 2)).collect()[0][0]

print(max_gpa, min_gpa, avg_gpa)

# Determining the students with the highest and lowest GPAs
max_student = df.filter(col("cumulative_gpa") == max_gpa).select("student_name").collect()[0][0]
min_student = df.filter(col("cumulative_gpa") == min_gpa).select("student_name").collect()[0][0]

# Counting the number of students with the rounded average GPA
avg_count = df.filter(col("cumulative_gpa") == avg_gpa).count()

# Creating a new DataFrame with the required schema
schema = ("top_name string, last_name string, avg_count int")
data = [[max_student, min_student, avg_count]]
final_df = spark.createDataFrame(data, schema)

# Displaying the new DataFrame
display(final_df)








